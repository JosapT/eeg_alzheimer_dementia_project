{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537c9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def can_stratify(labels):\n",
    "    \"\"\"Check if stratification is possible (each class has >= 2 samples)\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    return np.all(counts >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dba214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebbf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(EEGDataset, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e98d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini CNN model\n",
    "def build_mini_cnn(num_classes=2):\n",
    "    \"\"\"\n",
    "    Simple CNN for spectrogram classification.\n",
    "    Input: (batch, 19, 33, 7)\n",
    "    Output: logits (batch, num_classes)\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        # Block 1\n",
    "        nn.Conv2d(19, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Block 2\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Block 3\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        \n",
    "        # Global average pooling\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        \n",
    "        # Classifier\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(64, num_classes)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 clusters: ['data/clustered_k6/cluster_0', 'data/clustered_k6/cluster_1', 'data/clustered_k6/cluster_2', 'data/clustered_k6/cluster_3', 'data/clustered_k6/cluster_4', 'data/clustered_k6/cluster_5']\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing cluster_0\n",
      "============================================================\n",
      "  Loaded data shape: (3850, 4389), labels: (array([0, 1]), array([ 961, 2889]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 2594 epochs\n",
      "    Val:   595 epochs\n",
      "    Test:  661 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [3.9542682 1.3384933]\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [3.9542682 1.3384933]\n",
      "  Epoch [1/50] Train Loss: 53.9303, Train Acc: 0.5555 | Val Loss: 13.2691, Val Acc: 0.7412\n",
      "  Epoch [1/50] Train Loss: 53.9303, Train Acc: 0.5555 | Val Loss: 13.2691, Val Acc: 0.7412\n",
      "  Epoch [10/50] Train Loss: 13.5649, Train Acc: 0.9260 | Val Loss: 15.5836, Val Acc: 0.8555\n",
      "  Epoch [10/50] Train Loss: 13.5649, Train Acc: 0.9260 | Val Loss: 15.5836, Val Acc: 0.8555\n",
      "  Epoch [20/50] Train Loss: 2.1060, Train Acc: 0.9934 | Val Loss: 17.4842, Val Acc: 0.8689\n",
      "  Epoch [20/50] Train Loss: 2.1060, Train Acc: 0.9934 | Val Loss: 17.4842, Val Acc: 0.8689\n",
      "  Epoch [30/50] Train Loss: 4.4756, Train Acc: 0.9784 | Val Loss: 16.3872, Val Acc: 0.8840\n",
      "  Epoch [30/50] Train Loss: 4.4756, Train Acc: 0.9784 | Val Loss: 16.3872, Val Acc: 0.8840\n",
      "  Epoch [40/50] Train Loss: 6.4742, Train Acc: 0.9803 | Val Loss: 16.4222, Val Acc: 0.8538\n",
      "  Epoch [40/50] Train Loss: 6.4742, Train Acc: 0.9803 | Val Loss: 16.4222, Val Acc: 0.8538\n",
      "  Epoch [50/50] Train Loss: 1.0960, Train Acc: 0.9969 | Val Loss: 51.5650, Val Acc: 0.8504\n",
      "  Test Accuracy: 86.08%\n",
      "  Saved to models/cluster_models/cluster_0\n",
      "\n",
      "============================================================\n",
      "Processing cluster_1\n",
      "============================================================\n",
      "  Epoch [50/50] Train Loss: 1.0960, Train Acc: 0.9969 | Val Loss: 51.5650, Val Acc: 0.8504\n",
      "  Test Accuracy: 86.08%\n",
      "  Saved to models/cluster_models/cluster_0\n",
      "\n",
      "============================================================\n",
      "Processing cluster_1\n",
      "============================================================\n",
      "  Loaded data shape: (5129, 4389), labels: (array([0, 1]), array([2010, 3119]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 3501 epochs\n",
      "    Val:   842 epochs\n",
      "    Test:  786 epochs\n",
      "  Per-sample normalization complete\n",
      "  Loaded data shape: (5129, 4389), labels: (array([0, 1]), array([2010, 3119]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 3501 epochs\n",
      "    Val:   842 epochs\n",
      "    Test:  786 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [2.5205183 1.6576705]\n",
      "  Class weights: [2.5205183 1.6576705]\n",
      "  Epoch [1/50] Train Loss: 52.1605, Train Acc: 0.7592 | Val Loss: 6.5926, Val Acc: 0.8943\n",
      "  Epoch [1/50] Train Loss: 52.1605, Train Acc: 0.7592 | Val Loss: 6.5926, Val Acc: 0.8943\n",
      "  Epoch [10/50] Train Loss: 3.7190, Train Acc: 0.9894 | Val Loss: 2.3759, Val Acc: 0.9727\n",
      "  Epoch [10/50] Train Loss: 3.7190, Train Acc: 0.9894 | Val Loss: 2.3759, Val Acc: 0.9727\n",
      "  Epoch [20/50] Train Loss: 0.2712, Train Acc: 0.9994 | Val Loss: 2.9574, Val Acc: 0.9822\n",
      "  Epoch [20/50] Train Loss: 0.2712, Train Acc: 0.9994 | Val Loss: 2.9574, Val Acc: 0.9822\n",
      "  Epoch [30/50] Train Loss: 0.0021, Train Acc: 1.0000 | Val Loss: 4.2407, Val Acc: 0.9798\n",
      "  Epoch [30/50] Train Loss: 0.0021, Train Acc: 1.0000 | Val Loss: 4.2407, Val Acc: 0.9798\n",
      "  Epoch [40/50] Train Loss: 8.7239, Train Acc: 0.9783 | Val Loss: 2.5336, Val Acc: 0.9786\n",
      "  Epoch [40/50] Train Loss: 8.7239, Train Acc: 0.9783 | Val Loss: 2.5336, Val Acc: 0.9786\n",
      "  Epoch [50/50] Train Loss: 0.1644, Train Acc: 0.9997 | Val Loss: 2.9392, Val Acc: 0.9869\n",
      "  Test Accuracy: 97.84%\n",
      "  Saved to models/cluster_models/cluster_1\n",
      "\n",
      "============================================================\n",
      "Processing cluster_2\n",
      "============================================================\n",
      "  Epoch [50/50] Train Loss: 0.1644, Train Acc: 0.9997 | Val Loss: 2.9392, Val Acc: 0.9869\n",
      "  Test Accuracy: 97.84%\n",
      "  Saved to models/cluster_models/cluster_1\n",
      "\n",
      "============================================================\n",
      "Processing cluster_2\n",
      "============================================================\n",
      "  Loaded data shape: (24682, 4389), labels: (array([0, 1]), array([ 9153, 15529]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Loaded data shape: (24682, 4389), labels: (array([0, 1]), array([ 9153, 15529]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 17309 epochs\n",
      "    Val:   3759 epochs\n",
      "    Test:  3614 epochs\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 17309 epochs\n",
      "    Val:   3759 epochs\n",
      "    Test:  3614 epochs\n",
      "  Per-sample normalization complete\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [2.6740305 1.5973606]\n",
      "  Class weights: [2.6740305 1.5973606]\n",
      "  Epoch [1/50] Train Loss: 249.4492, Train Acc: 0.7785 | Val Loss: 35.1820, Val Acc: 0.8949\n",
      "  Epoch [1/50] Train Loss: 249.4492, Train Acc: 0.7785 | Val Loss: 35.1820, Val Acc: 0.8949\n",
      "  Epoch [10/50] Train Loss: 20.9352, Train Acc: 0.9867 | Val Loss: 18.8345, Val Acc: 0.9614\n",
      "  Epoch [10/50] Train Loss: 20.9352, Train Acc: 0.9867 | Val Loss: 18.8345, Val Acc: 0.9614\n",
      "  Epoch [20/50] Train Loss: 8.8059, Train Acc: 0.9947 | Val Loss: 16.1470, Val Acc: 0.9614\n",
      "  Epoch [20/50] Train Loss: 8.8059, Train Acc: 0.9947 | Val Loss: 16.1470, Val Acc: 0.9614\n",
      "  Epoch [30/50] Train Loss: 4.7764, Train Acc: 0.9973 | Val Loss: 14.0713, Val Acc: 0.9707\n",
      "  Epoch [30/50] Train Loss: 4.7764, Train Acc: 0.9973 | Val Loss: 14.0713, Val Acc: 0.9707\n",
      "  Epoch [40/50] Train Loss: 7.0007, Train Acc: 0.9958 | Val Loss: 17.1944, Val Acc: 0.9705\n",
      "  Epoch [40/50] Train Loss: 7.0007, Train Acc: 0.9958 | Val Loss: 17.1944, Val Acc: 0.9705\n",
      "  Epoch [50/50] Train Loss: 4.6624, Train Acc: 0.9974 | Val Loss: 23.3528, Val Acc: 0.9678\n",
      "  Epoch [50/50] Train Loss: 4.6624, Train Acc: 0.9974 | Val Loss: 23.3528, Val Acc: 0.9678\n",
      "  Test Accuracy: 96.79%\n",
      "  Saved to models/cluster_models/cluster_2\n",
      "\n",
      "============================================================\n",
      "Processing cluster_3\n",
      "============================================================\n",
      "  Loaded data shape: (227, 4389), labels: (array([0, 1]), array([209,  18]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Test Accuracy: 96.79%\n",
      "  Saved to models/cluster_models/cluster_2\n",
      "\n",
      "============================================================\n",
      "Processing cluster_3\n",
      "============================================================\n",
      "  Loaded data shape: (227, 4389), labels: (array([0, 1]), array([209,  18]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 162 epochs\n",
      "    Val:   33 epochs\n",
      "    Test:  32 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [ 1.1020408 10.8      ]\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 162 epochs\n",
      "    Val:   33 epochs\n",
      "    Test:  32 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [ 1.1020408 10.8      ]\n",
      "  Epoch [1/50] Train Loss: 4.1115, Train Acc: 0.9074 | Val Loss: 1.3043, Val Acc: 0.9394\n",
      "  Epoch [1/50] Train Loss: 4.1115, Train Acc: 0.9074 | Val Loss: 1.3043, Val Acc: 0.9394\n",
      "  Epoch [10/50] Train Loss: 2.5456, Train Acc: 0.9444 | Val Loss: 0.3419, Val Acc: 1.0000\n",
      "  Epoch [10/50] Train Loss: 2.5456, Train Acc: 0.9444 | Val Loss: 0.3419, Val Acc: 1.0000\n",
      "  Epoch [20/50] Train Loss: 0.0409, Train Acc: 1.0000 | Val Loss: 0.1155, Val Acc: 1.0000\n",
      "  Epoch [20/50] Train Loss: 0.0409, Train Acc: 1.0000 | Val Loss: 0.1155, Val Acc: 1.0000\n",
      "  Epoch [30/50] Train Loss: 0.0136, Train Acc: 1.0000 | Val Loss: 0.0634, Val Acc: 1.0000\n",
      "  Epoch [30/50] Train Loss: 0.0136, Train Acc: 1.0000 | Val Loss: 0.0634, Val Acc: 1.0000\n",
      "  Epoch [40/50] Train Loss: 0.0066, Train Acc: 1.0000 | Val Loss: 0.1762, Val Acc: 0.9697\n",
      "  Epoch [40/50] Train Loss: 0.0066, Train Acc: 1.0000 | Val Loss: 0.1762, Val Acc: 0.9697\n",
      "  Epoch [50/50] Train Loss: 0.0023, Train Acc: 1.0000 | Val Loss: 0.1799, Val Acc: 0.9697\n",
      "  Test Accuracy: 96.88%\n",
      "  Saved to models/cluster_models/cluster_3\n",
      "\n",
      "============================================================\n",
      "Processing cluster_4\n",
      "============================================================\n",
      "  Loaded data shape: (2557, 4389), labels: (array([0, 1]), array([ 844, 1713]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 1801 epochs\n",
      "    Val:   364 epochs\n",
      "    Test:  392 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [3.0268908 1.4933665]\n",
      "  Epoch [50/50] Train Loss: 0.0023, Train Acc: 1.0000 | Val Loss: 0.1799, Val Acc: 0.9697\n",
      "  Test Accuracy: 96.88%\n",
      "  Saved to models/cluster_models/cluster_3\n",
      "\n",
      "============================================================\n",
      "Processing cluster_4\n",
      "============================================================\n",
      "  Loaded data shape: (2557, 4389), labels: (array([0, 1]), array([ 844, 1713]))\n",
      "  Patient-level split:\n",
      "    Train: 32 patients\n",
      "    Val:   7 patients\n",
      "    Test:  8 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 1801 epochs\n",
      "    Val:   364 epochs\n",
      "    Test:  392 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [3.0268908 1.4933665]\n",
      "  Epoch [1/50] Train Loss: 33.6460, Train Acc: 0.6935 | Val Loss: 4.2511, Val Acc: 0.8709\n",
      "  Epoch [1/50] Train Loss: 33.6460, Train Acc: 0.6935 | Val Loss: 4.2511, Val Acc: 0.8709\n",
      "  Epoch [10/50] Train Loss: 1.7307, Train Acc: 0.9900 | Val Loss: 2.3215, Val Acc: 0.9588\n",
      "  Epoch [10/50] Train Loss: 1.7307, Train Acc: 0.9900 | Val Loss: 2.3215, Val Acc: 0.9588\n",
      "  Epoch [20/50] Train Loss: 0.0686, Train Acc: 1.0000 | Val Loss: 5.0343, Val Acc: 0.9588\n",
      "  Epoch [20/50] Train Loss: 0.0686, Train Acc: 1.0000 | Val Loss: 5.0343, Val Acc: 0.9588\n",
      "  Epoch [30/50] Train Loss: 0.0063, Train Acc: 1.0000 | Val Loss: 6.4548, Val Acc: 0.9560\n",
      "  Epoch [30/50] Train Loss: 0.0063, Train Acc: 1.0000 | Val Loss: 6.4548, Val Acc: 0.9560\n",
      "  Epoch [40/50] Train Loss: 1.2358, Train Acc: 0.9911 | Val Loss: 4.7610, Val Acc: 0.9560\n",
      "  Epoch [40/50] Train Loss: 1.2358, Train Acc: 0.9911 | Val Loss: 4.7610, Val Acc: 0.9560\n",
      "  Epoch [50/50] Train Loss: 0.2700, Train Acc: 0.9983 | Val Loss: 8.7745, Val Acc: 0.9478\n",
      "  Epoch [50/50] Train Loss: 0.2700, Train Acc: 0.9983 | Val Loss: 8.7745, Val Acc: 0.9478\n",
      "  Test Accuracy: 95.41%\n",
      "  Saved to models/cluster_models/cluster_4\n",
      "\n",
      "============================================================\n",
      "Processing cluster_5\n",
      "============================================================\n",
      "  Loaded data shape: (64, 4389), labels: (array([0, 1]), array([25, 39]))\n",
      "  Patient-level split:\n",
      "    Train: 22 patients\n",
      "    Val:   5 patients\n",
      "    Test:  5 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 42 epochs\n",
      "    Val:   12 epochs\n",
      "    Test:  10 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [2.625     1.6153846]\n",
      "  Test Accuracy: 95.41%\n",
      "  Saved to models/cluster_models/cluster_4\n",
      "\n",
      "============================================================\n",
      "Processing cluster_5\n",
      "============================================================\n",
      "  Loaded data shape: (64, 4389), labels: (array([0, 1]), array([25, 39]))\n",
      "  Patient-level split:\n",
      "    Train: 22 patients\n",
      "    Val:   5 patients\n",
      "    Test:  5 patients\n",
      "  Epoch counts after concatenation:\n",
      "    Train: 42 epochs\n",
      "    Val:   12 epochs\n",
      "    Test:  10 epochs\n",
      "  Per-sample normalization complete\n",
      "  Class weights: [2.625     1.6153846]\n",
      "  Epoch [1/50] Train Loss: 1.3929, Train Acc: 0.4048 | Val Loss: 0.6842, Val Acc: 0.5000\n",
      "  Epoch [1/50] Train Loss: 1.3929, Train Acc: 0.4048 | Val Loss: 0.6842, Val Acc: 0.5000\n",
      "  Epoch [10/50] Train Loss: 1.3686, Train Acc: 0.6429 | Val Loss: 0.6910, Val Acc: 0.6667\n",
      "  Epoch [10/50] Train Loss: 1.3686, Train Acc: 0.6429 | Val Loss: 0.6910, Val Acc: 0.6667\n",
      "  Epoch [20/50] Train Loss: 1.0661, Train Acc: 0.8333 | Val Loss: 0.6632, Val Acc: 0.5000\n",
      "  Epoch [20/50] Train Loss: 1.0661, Train Acc: 0.8333 | Val Loss: 0.6632, Val Acc: 0.5000\n",
      "  Epoch [30/50] Train Loss: 0.3208, Train Acc: 0.9524 | Val Loss: 0.7350, Val Acc: 0.8333\n",
      "  Epoch [30/50] Train Loss: 0.3208, Train Acc: 0.9524 | Val Loss: 0.7350, Val Acc: 0.8333\n",
      "  Epoch [40/50] Train Loss: 0.0524, Train Acc: 1.0000 | Val Loss: 0.9456, Val Acc: 0.7500\n",
      "  Epoch [40/50] Train Loss: 0.0524, Train Acc: 1.0000 | Val Loss: 0.9456, Val Acc: 0.7500\n",
      "  Epoch [50/50] Train Loss: 0.0061, Train Acc: 1.0000 | Val Loss: 1.6348, Val Acc: 0.6667\n",
      "  Test Accuracy: 70.00%\n",
      "  Saved to models/cluster_models/cluster_5\n",
      "\n",
      "============================================================\n",
      "All clusters processed!\n",
      "============================================================\n",
      "  Epoch [50/50] Train Loss: 0.0061, Train Acc: 1.0000 | Val Loss: 1.6348, Val Acc: 0.6667\n",
      "  Test Accuracy: 70.00%\n",
      "  Saved to models/cluster_models/cluster_5\n",
      "\n",
      "============================================================\n",
      "All clusters processed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Process each cluster\n",
    "cluster_base_dir = 'data/clustered_k6'\n",
    "output_base_dir = 'models/cluster_models'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "CHANNELS = 19\n",
    "FREQ = 33\n",
    "TIME = 7\n",
    "FLATTENED_DIM = CHANNELS * FREQ * TIME\n",
    "eps = 1e-8\n",
    "\n",
    "cluster_dirs = sorted([d for d in glob.glob(os.path.join(cluster_base_dir, 'cluster_*')) if os.path.isdir(d)])\n",
    "print(f'Found {len(cluster_dirs)} clusters: {cluster_dirs}\\n')\n",
    "\n",
    "for cluster_dir in cluster_dirs:\n",
    "    cluster_name = os.path.basename(cluster_dir)\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Processing {cluster_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Load per-patient files from cluster\n",
    "    X_parts, y_parts, patient_ids = [], [], []\n",
    "    for xf in sorted(glob.glob(os.path.join(cluster_dir, 'sub-*_X.npy'))):\n",
    "        pid = os.path.basename(xf).split('_')[0]  # e.g., 'sub-001'\n",
    "        Xp = np.load(xf)\n",
    "        yp = np.load(os.path.join(cluster_dir, f'{pid}_y.npy'))\n",
    "        X_parts.append(Xp)\n",
    "        y_parts.append(yp)\n",
    "        patient_ids.extend([pid] * len(yp))  # associate each epoch with patient ID\n",
    "    \n",
    "    if len(X_parts) == 0:\n",
    "        print(f'No data found in {cluster_dir}, skipping...')\n",
    "        continue\n",
    "    \n",
    "    # Concatenate all patients in this cluster (epochs from all patients)\n",
    "    X_all = np.concatenate(X_parts, axis=0)\n",
    "    y_all = np.concatenate(y_parts, axis=0)\n",
    "    patient_ids = np.array(patient_ids)\n",
    "    \n",
    "    print(f'  Loaded data shape: {X_all.shape}, labels: {np.unique(y_all, return_counts=True)}')\n",
    "    \n",
    "    # Reshape if flattened\n",
    "    if X_all.ndim == 2 and X_all.shape[1] == FLATTENED_DIM:\n",
    "        X_all = X_all.reshape((-1, CHANNELS, FREQ, TIME))\n",
    "    \n",
    "    # --- Patient-level split (avoids data leakage) ---\n",
    "    # Get unique patients and their labels\n",
    "    unique_patients = np.unique(patient_ids)\n",
    "    if unique_patients.size < 2:\n",
    "        print(f'  Only {unique_patients.size} patient(s) in cluster; skipping patient-level split. Skipping cluster.')\n",
    "        continue\n",
    "    patient_labels = np.array([y_all[np.where(patient_ids == pid)[0][0]] for pid in unique_patients])\n",
    "    \n",
    "    # First split: train vs temp (use stratify only when possible)\n",
    "    strat_1 = patient_labels if can_stratify(patient_labels) else None\n",
    "    train_patient_ids, temp_patient_ids = train_test_split(\n",
    "        unique_patients,\n",
    "        test_size=0.30,\n",
    "        stratify=strat_1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs test (50/50 of temp)\n",
    "    temp_labels = np.array([y_all[np.where(patient_ids == pid)[0][0]] for pid in temp_patient_ids])\n",
    "    strat_2 = temp_labels if can_stratify(temp_labels) else None\n",
    "    # If temp_patient_ids has fewer than 2 patients, fall back: assign all to val (or test) appropriately\n",
    "    if len(temp_patient_ids) < 2:\n",
    "        # Move one patient from train to val if possible, otherwise assign all temp to val\n",
    "        if len(train_patient_ids) >= 1:\n",
    "            # Move last train patient into val to ensure at least one patient in val/test\n",
    "            moved = train_patient_ids[-1]\n",
    "            train_patient_ids = train_patient_ids[:-1]\n",
    "            val_patient_ids = np.array([moved])\n",
    "            test_patient_ids = np.array([])\n",
    "        else:\n",
    "            val_patient_ids = temp_patient_ids\n",
    "            test_patient_ids = np.array([])\n",
    "    else:\n",
    "        val_patient_ids, test_patient_ids = train_test_split(\n",
    "            temp_patient_ids,\n",
    "            test_size=0.50,\n",
    "            stratify=strat_2,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    print(f'  Patient-level split:')\n",
    "    print(f'    Train: {len(train_patient_ids)} patients')\n",
    "    print(f'    Val:   {len(val_patient_ids)} patients')\n",
    "    print(f'    Test:  {len(test_patient_ids)} patients')\n",
    "    \n",
    "    # Extract epochs for each split\n",
    "    train_mask = np.isin(patient_ids, train_patient_ids)\n",
    "    val_mask = np.isin(patient_ids, val_patient_ids)\n",
    "    test_mask = np.isin(patient_ids, test_patient_ids)\n",
    "    \n",
    "    X_train = X_all[train_mask]\n",
    "    y_train = y_all[train_mask]\n",
    "    \n",
    "    X_val = X_all[val_mask]\n",
    "    y_val = y_all[val_mask]\n",
    "    \n",
    "    X_test = X_all[test_mask]\n",
    "    y_test = y_all[test_mask]\n",
    "    \n",
    "    print(f'  Epoch counts after concatenation:')\n",
    "    print(f'    Train: {len(X_train)} epochs')\n",
    "    print(f'    Val:   {len(X_val)} epochs')\n",
    "    print(f'    Test:  {len(X_test)} epochs')\n",
    "    \n",
    "    # Per-sample normalization\n",
    "    for i in range(X_train.shape[0]):\n",
    "        m = X_train[i].mean()\n",
    "        s = X_train[i].std()\n",
    "        X_train[i] = (X_train[i] - m) / (s + eps)\n",
    "    \n",
    "    for i in range(X_val.shape[0]):\n",
    "        m = X_val[i].mean()\n",
    "        s = X_val[i].std()\n",
    "        X_val[i] = (X_val[i] - m) / (s + eps)\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        m = X_test[i].mean()\n",
    "        s = X_test[i].std()\n",
    "        X_test[i] = (X_test[i] - m) / (s + eps)\n",
    "    \n",
    "    print(f'  Per-sample normalization complete')\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Build model\n",
    "    num_classes = len(np.unique(y_all))\n",
    "    model = build_mini_cnn(num_classes=num_classes).to(device)\n",
    "    \n",
    "    # Class weights\n",
    "    unique_y_train, counts_y_train = np.unique(y_train, return_counts=True)\n",
    "    total_train = len(y_train)\n",
    "    class_weights_dict = {label: total_train / (count + 1e-8) for label, count in zip(unique_y_train, counts_y_train)}\n",
    "    weights_list = [class_weights_dict.get(i, 1.0) for i in range(num_classes)]\n",
    "    weights = torch.tensor(weights_list, dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    \n",
    "    print(f'  Class weights: {weights.cpu().numpy()}')\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    \n",
    "    # Training\n",
    "    num_epochs = 50\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "        \n",
    "        train_acc = correct / total if total > 0 else 0\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                val_correct += (preds == batch_y).sum().item()\n",
    "                val_total += batch_y.size(0)\n",
    "        \n",
    "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            out_dir = os.path.join(output_base_dir, cluster_name)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, f'best_model_{best_val_acc:.4f}.pth'))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'  Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Test\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / test_total if test_total > 0 else 0\n",
    "    print(f'  Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "    out_dir = os.path.join(output_base_dir, cluster_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(os.path.join(out_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    print(f'  Saved to {out_dir}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('All clusters processed!')\n",
    "print(f'{\"=\"*60}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_184a_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
